[
  {
    "objectID": "contributions.html",
    "href": "contributions.html",
    "title": "Contributions",
    "section": "",
    "text": "Adnan Baig | BSc in Economics\nAnna Jin | BSc in Economics\nHailey Stevens | BA in History"
  },
  {
    "objectID": "contributions.html#team-members",
    "href": "contributions.html#team-members",
    "title": "Contributions",
    "section": "",
    "text": "Adnan Baig | BSc in Economics\nAnna Jin | BSc in Economics\nHailey Stevens | BA in History"
  },
  {
    "objectID": "contributions.html#guidance",
    "href": "contributions.html#guidance",
    "title": "Contributions",
    "section": "Guidance",
    "text": "Guidance\n\nAlexander Soldatkin\nJon Cardoso-Silva"
  },
  {
    "objectID": "visualisations.html#lda",
    "href": "visualisations.html#lda",
    "title": "Visualisations",
    "section": "LDA",
    "text": "LDA"
  },
  {
    "objectID": "visualisations.html#sentiment-analysis",
    "href": "visualisations.html#sentiment-analysis",
    "title": "Visualisations",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis"
  },
  {
    "objectID": "collection.html",
    "href": "collection.html",
    "title": "Data Collection",
    "section": "",
    "text": "Reddit API Limitations: The inability to filter posts by date with the Reddit API hampers historical data collection.\nPushshift API Access Restrictions: Changes in the Pushshift API’s terms of use restrict its availability for research, limiting access to historical subreddit data.\nScraping Date-Specific Data: The need to find alternative methods to bypass the Reddit API’s date filtering limitations complicates data collection.\nWeb Scraping Tool Limitations: Using Selenium for scraping reveals a cap on data volume, with a maximum of 1000 recent posts being retrievable at a time.\nAPI Access Challenges: Failed attempts to use another’s Pushshift API access highlight the difficulties in obtaining necessary permissions for data access.\nData Format Navigation: Requesting a CSV version of Reddit API data points to the challenges of managing and utilizing the provided data formats efficiently.",
    "crumbs": [
      "Data Collection"
    ]
  },
  {
    "objectID": "collection.html#rwallstreetbets-posts",
    "href": "collection.html#rwallstreetbets-posts",
    "title": "Data Collection",
    "section": "",
    "text": "Reddit API Limitations: The inability to filter posts by date with the Reddit API hampers historical data collection.\nPushshift API Access Restrictions: Changes in the Pushshift API’s terms of use restrict its availability for research, limiting access to historical subreddit data.\nScraping Date-Specific Data: The need to find alternative methods to bypass the Reddit API’s date filtering limitations complicates data collection.\nWeb Scraping Tool Limitations: Using Selenium for scraping reveals a cap on data volume, with a maximum of 1000 recent posts being retrievable at a time.\nAPI Access Challenges: Failed attempts to use another’s Pushshift API access highlight the difficulties in obtaining necessary permissions for data access.\nData Format Navigation: Requesting a CSV version of Reddit API data points to the challenges of managing and utilizing the provided data formats efficiently.",
    "crumbs": [
      "Data Collection"
    ]
  },
  {
    "objectID": "collection.html#gamestop-stock-prices",
    "href": "collection.html#gamestop-stock-prices",
    "title": "Data Collection",
    "section": "GameStop Stock Prices",
    "text": "GameStop Stock Prices\n\nCollection Process\n\n\n\nChallenges Faced\n\nAPI Key Registration: Obtaining an API key requires registration, which may involve sharing personal information and adhering to specific use cases.\nRate Limits and Quotas: Alpha Vantage imposes rate limits that can slow the data collection process, a significant consideration for large datasets.",
    "crumbs": [
      "Data Collection"
    ]
  },
  {
    "objectID": "data_overview.html#interactive-plot",
    "href": "data_overview.html#interactive-plot",
    "title": "Data Overview",
    "section": "Interactive Plot",
    "text": "Interactive Plot",
    "crumbs": [
      "Data Overview"
    ]
  },
  {
    "objectID": "data_overview.html#interactive-timeline",
    "href": "data_overview.html#interactive-timeline",
    "title": "Data Overview",
    "section": "Interactive Timeline",
    "text": "Interactive Timeline",
    "crumbs": [
      "Data Overview"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dabbling in Data",
    "section": "",
    "text": "2020-21 GameStop Short Squeeze: an analysis of the profound impact of social meedia communities on the price of GameStop stock."
  },
  {
    "objectID": "index.html#project-overview",
    "href": "index.html#project-overview",
    "title": "Dabbling in Data",
    "section": "Project Overview",
    "text": "Project Overview\nWelcome to Dabbling in Data, a project which delves into the GameStop (GME) short squeeze phenomenon. Our primary objective is to shed light on the causation and degree of influence the Reddit community, r/wallstreetbets, had on the stock price of GameStop. We conducted an analysis of GME stock data in relation to r/wallstreetbets posts, over the period December 2020 to March 2021."
  },
  {
    "objectID": "index.html#our-motivation",
    "href": "index.html#our-motivation",
    "title": "Dabbling in Data",
    "section": "Our Motivation",
    "text": "Our Motivation\nOur project stems from a fascination with this event and its disruption to traditional financial norms, its empowerment of online communities, and its ensuing impact on regulation. Through our analysis, we aim to reveal the role of online communities on stock market behaviour."
  },
  {
    "objectID": "index.html#contextualising-our-project",
    "href": "index.html#contextualising-our-project",
    "title": "Dabbling in Data",
    "section": "Contextualising Our Project",
    "text": "Contextualising Our Project\nThe GameStop short squeeze in early 2021 saw a remarkable surge in the stock price of GameStop (GME). This surge was orchestrated by retail investors, particularly through the Reddit community r/wallstreetbets. As these investors bought GME shares en masse, it forced hedge funds and institutional investors, who had bet against the stock, to cover their positions, causing GME’s price to skyrocket."
  },
  {
    "objectID": "cleaning.html",
    "href": "cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Formatting: Converting ‘score’ and ‘num_comments’ fields from their original format to integers for quantitative analysis presents a fundamental data type consistency challenge.\nAlignment: Formatting the ‘date’ column to ‘datetime64’ to ensure compatibility and precise alignment with GameStop (GME) stock price data for time series analysis.\nAggregation: Using the groupby method to aggregate posts by date requires careful handling to ensure accurate summarization of data points, like total posts, average scores, or comments per day.\nCreating Dataframe: The overall process of transforming raw subreddit data into a cleaned and structured dataframe ready for analysis involves several preprocessing steps, including handling missing values, removing duplicates, and standardizing data formats.",
    "crumbs": [
      "Data Cleaning"
    ]
  },
  {
    "objectID": "cleaning.html#rwallstreetbets-posts",
    "href": "cleaning.html#rwallstreetbets-posts",
    "title": "Data Cleaning",
    "section": "",
    "text": "Formatting: Converting ‘score’ and ‘num_comments’ fields from their original format to integers for quantitative analysis presents a fundamental data type consistency challenge.\nAlignment: Formatting the ‘date’ column to ‘datetime64’ to ensure compatibility and precise alignment with GameStop (GME) stock price data for time series analysis.\nAggregation: Using the groupby method to aggregate posts by date requires careful handling to ensure accurate summarization of data points, like total posts, average scores, or comments per day.\nCreating Dataframe: The overall process of transforming raw subreddit data into a cleaned and structured dataframe ready for analysis involves several preprocessing steps, including handling missing values, removing duplicates, and standardizing data formats.",
    "crumbs": [
      "Data Cleaning"
    ]
  },
  {
    "objectID": "cleaning.html#gamestop-stock-prices",
    "href": "cleaning.html#gamestop-stock-prices",
    "title": "Data Cleaning",
    "section": "GameStop Stock Prices",
    "text": "GameStop Stock Prices\n\nCleaning Process\n\n\n\nChallenges Faced\nCreating Dataframe: The overarching challenge of transforming the raw API data into a cleaned dataframe involves multiple sub-tasks.\n\nEnsuring data consistency and accuracy after type conversion.\nHandling any missing or anomalous data points that could skew analysis.\nStructuring the data in a way that aligns with analytical goals, ensuring it’s ready for analysis or visualization.",
    "crumbs": [
      "Data Cleaning"
    ]
  }
]