{"cells":[{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["import os\n","import json\n","import requests\n","import requests.auth\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import time\n","import datetime"]},{"cell_type":"markdown","metadata":{},"source":["Reading the Reddit credentials from the json file in gitignore to prevent sensitive information being exposed on GitHub."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["credentials_file_path = \"credentials.json\"\n","\n","# open the file and load the data into a variable\n","with open(credentials_file_path, \"r\") as f:\n","    credentials = json.load(f)\n","\n","#STILL NEED TO DO THE VIRTUAL ENVIRONMENT STUFF"]},{"cell_type":"markdown","metadata":{},"source":["Obtainng a token\n","- Setting up credentials\n","- Sending the request"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# We will still use the requests library, only this time we have to set up authentication parameters first\n","client_auth = requests.auth.HTTPBasicAuth(credentials[\"app_client_id\"], credentials[\"app_client_secret\"])\n","\n","# You also need to send, via HTTP POST, your Reddit username and password.\n","post_data = {\"grant_type\": \"password\", \"username\": credentials[\"reddit_username\"], \"password\": credentials[\"reddit_password\"]}\n","\n","# Just like Wikimedia, Reddit API also requests that we self-identify ourselves in the User-Agent.\n","headers = {\"User-Agent\": f\"LSE DS105A API practice by {credentials['reddit_username']}\"}\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["{'access_token': 'eyJhbGciOiJSUzI1NiIsImtpZCI6IlNIQTI1NjpzS3dsMnlsV0VtMjVmcXhwTU40cWY4MXE2OWFFdWFyMnpLMUdhVGxjdWNZIiwidHlwIjoiSldUIn0.eyJzdWIiOiJ1c2VyIiwiZXhwIjoxNzAxMTc0NzM2Ljk1MjkyMSwiaWF0IjoxNzAxMDg4MzM2Ljk1MjkyMSwianRpIjoiaFdwRkhzMzQ2SmpoRXJrUGVRMDJVdnUtMmlBeEpBIiwiY2lkIjoiVVlINVVubExHSTJUWG1QTWY0WHdPUSIsImxpZCI6InQyX29uaXdoc2owMCIsImFpZCI6InQyX29uaXdoc2owMCIsImxjYSI6MTcwMDk0NDE0NzEyNywic2NwIjoiZUp5S1Z0SlNpZ1VFQUFEX193TnpBU2MiLCJmbG8iOjl9.f0-PJFTQ8A1gB897JkTjavn80yQLYY_2IHdBzh2nY7KxTefCxGJpmJ2AzSXcTl5eAacJt1ce9wlNcD6YbxIfZYJ6WLPF_KWNTlMHKO1yq2asoSX5DJlAeVhkUKDWasoQ8oIDyLhJUR8_om5u3GJQHAKPaT_v_jYUNToIdxREs-Zbaz8QjFztpn--L_FibK3squ6l1E87LoRT0nt_YwRMIuYH741L7pZpfzJBufTYHvcgRtlY7MNODkEjgu5UcFp4coyN2QUsh42qB2JXhPVkFoskMeO-safKpx0e3mtJIOC16f8-s5aEPM38uIY5ylwndR40nUNQ5pqOHUAO-B4T4w',\n"," 'token_type': 'bearer',\n"," 'expires_in': 86400,\n"," 'scope': '*'}"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# From their documentation, I learned this is the endpoint I need\n","ACCESS_TOKEN_ENDPOINT = \"https://www.reddit.com/api/v1/access_token\"\n","\n","# This time we are sending a HTTP POST instead of a HTTP GET\n","response = requests.post(ACCESS_TOKEN_ENDPOINT, auth=client_auth, data=post_data, headers=headers)\n","response.json()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["#Setup/authentification, and defining subreddit\n","access_token = response.json()['access_token']\n","\n","headers = {\n","    \"Authorization\": f\"bearer {access_token}\",\n","    \"User-Agent\": \"LSE DS105A API practice by yourusername\"\n","}\n","\n","subreddit = 'wallstreetbets'\n","url = f'https://oauth.reddit.com/r/{subreddit}/'"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["response = requests.get(url, headers=headers)\n","posts = response.json()\n","\n","# Save the response into a list or other data structure for later use\n","posts_data = []\n","for post in posts['data']['children']:\n","    posts_data.append(post['data'])\n","    \n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>title</th>\n","      <th>score</th>\n","      <th>id</th>\n","      <th>url</th>\n","      <th>created</th>\n","      <th>comments_num</th>\n","      <th>upvote_ratio</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Daily Discussion Thread for November 27, 2023</td>\n","      <td>10</td>\n","      <td>1850f23</td>\n","      <td>https://www.reddit.com/r/wallstreetbets/commen...</td>\n","      <td>1.701083e+09</td>\n","      <td>147</td>\n","      <td>0.82</td>\n","      <td>Join [WSB's community voice chat](https://disc...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Most Anticipated Earnings Releases for the wee...</td>\n","      <td>124</td>\n","      <td>180z98f</td>\n","      <td>https://i.redd.it/reploj8aft1c1.jpg</td>\n","      <td>1.700622e+09</td>\n","      <td>147</td>\n","      <td>0.98</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Merica is cooked ðŸ˜­Puts on Target</td>\n","      <td>7770</td>\n","      <td>184lfps</td>\n","      <td>https://v.redd.it/d8p45w6mfr2c1</td>\n","      <td>1.701034e+09</td>\n","      <td>585</td>\n","      <td>0.91</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>SEC Investigation Looms For OpenAI CEO Sam Alt...</td>\n","      <td>814</td>\n","      <td>184v2rc</td>\n","      <td>https://www.halt.org/sec-investigation-looms-f...</td>\n","      <td>1.701061e+09</td>\n","      <td>27</td>\n","      <td>0.99</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Puts on best buy?</td>\n","      <td>14257</td>\n","      <td>184b4xn</td>\n","      <td>https://v.redd.it/ulj4vv726p2c1</td>\n","      <td>1.701007e+09</td>\n","      <td>2940</td>\n","      <td>0.91</td>\n","      <td></td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               title  score       id  \\\n","0      Daily Discussion Thread for November 27, 2023     10  1850f23   \n","1  Most Anticipated Earnings Releases for the wee...    124  180z98f   \n","2                   Merica is cooked ðŸ˜­Puts on Target   7770  184lfps   \n","3  SEC Investigation Looms For OpenAI CEO Sam Alt...    814  184v2rc   \n","4                                  Puts on best buy?  14257  184b4xn   \n","\n","                                                 url       created  \\\n","0  https://www.reddit.com/r/wallstreetbets/commen...  1.701083e+09   \n","1                https://i.redd.it/reploj8aft1c1.jpg  1.700622e+09   \n","2                    https://v.redd.it/d8p45w6mfr2c1  1.701034e+09   \n","3  https://www.halt.org/sec-investigation-looms-f...  1.701061e+09   \n","4                    https://v.redd.it/ulj4vv726p2c1  1.701007e+09   \n","\n","   comments_num  upvote_ratio  \\\n","0           147          0.82   \n","1           147          0.98   \n","2           585          0.91   \n","3            27          0.99   \n","4          2940          0.91   \n","\n","                                             content  \n","0  Join [WSB's community voice chat](https://disc...  \n","1                                                     \n","2                                                     \n","3                                                     \n","4                                                     "]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["import time\n","import datetime\n","\n","# Function to convert a date to Unix timestamp\n","def to_unix_time(year, month, day):\n","    return int(time.mktime(datetime.datetime(year, month, day).timetuple()))\n","\n","# Define the start and end of the period you're interested in\n","start_time = to_unix_time(2021, 1, 1)\n","end_time = to_unix_time(2021, 3, 31)\n","\n","posts_details = []  # Store all posts details here\n","url = f'https://oauth.reddit.com/r/{subreddit}/'\n","\n","params = {\n","    'after': '',        # Placeholder for the pagination\n","    'before': end_time, # Fetch posts before this date\n","    'limit': 100        # Maximum number of posts per request (up to 100)\n","}\n","\n","while True:\n","    # Fetch posts\n","    response = requests.get(url, headers=headers, params=params)\n","    data = response.json()\n","    posts = data['data']['children']\n","    \n","    # Check if the posts are in the specified time range\n","    for post in posts:\n","        created_utc = post['data']['created_utc']\n","        if created_utc < start_time:\n","            # Break the loop if a post before the start time is encountered\n","            break\n","        \n","        # Extract details from posts\n","        post_info = {\n","            'title': post['data']['title'],\n","            'score': post['data']['score'],\n","            'id': post['data']['id'],\n","            'url': post['data']['url'],\n","            'created': created_utc,\n","            'comments_num': post['data']['num_comments'],\n","            'upvote_ratio': post['data']['upvote_ratio'],\n","            'content': post['data'].get('selftext', '')\n","        }\n","        posts_details.append(post_info)\n","    \n","    # Get the ID of the last post in this batch\n","    after = data['data']['after']\n","    \n","    # Update the parameters for the next request\n","    params['after'] = after\n","    \n","    # Sleep for a short while to avoid hitting the API rate limit\n","    time.sleep(1)\n","    \n","    # Break the loop if there are no more posts or if we've gone past the start time\n","    if after is None or created_utc < start_time:\n","        break\n","\n","# Convert the list of posts to a DataFrame\n","df_posts = pd.DataFrame(posts_details)\n","df_posts.head()\n"]},{"cell_type":"markdown","metadata":{},"source":["use pushshift api to do it with the correct dates?????"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["import requests\n","import time\n","from datetime import datetime\n","\n","# Function to convert a date to a Unix timestamp\n","def to_unix_time(dt):\n","    return int(time.mktime(dt.timetuple()))\n","\n","start_date = datetime(2021, 1, 1)\n","end_date = datetime(2021, 3, 31)\n","\n","# Convert dates to Unix timestamps\n","start_time = to_unix_time(start_date)\n","end_time = to_unix_time(end_date)\n","\n","url = 'https://api.pushshift.io/reddit/search/submission/'\n","\n","# Initialize the after variable to the start time\n","after = start_time\n","\n","all_posts = []\n","\n","while True:\n","    # Query Pushshift in batches\n","    params = {\n","        'subreddit': 'wallstreetbets',\n","        'size': 100,\n","        'after': after,\n","        'before': end_time\n","    }\n","    \n","    response = requests.get(url, params=params)\n","    if response.status_code != 200:\n","        break  # If there's an error, exit the loop\n","    \n","    data = response.json()['data']\n","    if not data:  # If no more posts are returned, exit the loop\n","        break\n","    \n","    all_posts.extend(data)\n","    \n","    # Get the timestamp of the last retrieved post and use it as the next 'after'\n","    after = data[-1]['created_utc']\n","    \n","    # Optional delay between requests to not overload the server\n","    time.sleep(1)\n","\n","# Now you can process and store the all_posts list as needed\n","\n","# Define the fields you want to extract\n","fields = ['title', 'selftext', 'created_utc', 'author', 'score', 'num_comments']\n","\n","# Define the file path\n","file_path = 'wallstreetbets_posts.json'\n","\n","# Open a file in write mode\n","with open(file_path, 'w', encoding='utf-8') as file:\n","    # Write the list of posts to the file in JSON format\n","    json.dump(all_posts, file, ensure_ascii=False, indent=4)\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["import csv\n","\n","# Define the fields you want to extract\n","fields = ['title', 'selftext', 'created_utc', 'author', 'score', 'num_comments']\n","\n","# Open a CSV file to write the data\n","with open('wallstreetbets_posts.csv', 'w', newline='', encoding='utf-8') as csvfile:\n","    writer = csv.DictWriter(csvfile, fieldnames=fields)\n","\n","    # Write the header\n","    writer.writeheader()\n","\n","    # Iterate over all posts and write the relevant data\n","    for post in all_posts:\n","        # Create a dictionary for each post with only the fields you're interested in\n","        filtered_post = {field: post[field] for field in fields if field in post}\n","        writer.writerow(filtered_post)\n"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":2}
